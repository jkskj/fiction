# 小说续写

## 爬虫

requests是同步请求库，用来爬取目录。

aiohttp是异步请求库，用来爬取每章内容。

导入***asyncio***、***aiohttp***（用于http请求）和 ***aiofiles***（用于文件读写操作）库。

关于asyncio的一些关键字的说明：

- **event_loop** 事件循环：程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数。
- **coroutine** 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。
- **task** 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含了任务的各种状态。
- **future** : 代表将来执行或没有执行的任务的结果。它和task上没有本质上的区别。
- **async/await** 关键字：python3.5用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。

*爬虫实战可参考* [第17讲：aiohttp 异步爬虫实战](https://blog.csdn.net/weixin_38819889/article/details/108632640)

*aiohttp文档* [英文](https://docs.aiohttp.org/) [中文](https://www.cntofu.com/book/127/aiohttp%E6%96%87%E6%A1%A3/Introduce.md)

## 训练

使用MASK进行训练，所以模型直接使用BertForMaskedLM。

### UniLM

论文地址：[Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)

UNILM全名Unified Language Model Pre-training for
Natural Language Understanding and Generation，
统一语言模型预训练。它仍以bert为模型结构，利用三种MASK机制，
将单向、双向以及sequence to sequence三种语言模型统一起来。

具体请参考:

- [Unilm模型的摘要任务实践](https://zhuanlan.zhihu.com/p/112391971)
- [UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation](https://adaning.github.io/posts/14266.html)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://adaning.github.io/posts/14266.html)

### 构建attention mask矩阵

因为这次是做小说生成，所以attention mask矩阵构建策略为Sequence-to-Sequence LM。

``` Python
def compute_attention_mask(segment_ids):
    idxs = torch.cumsum(segment_ids, dim=1)
    mask = idxs[:, None, :] <= idxs[:, :, None]
    return mask
```

其他的策略可参考[unilm三种模型结构中mask矩阵的代码实现](https://blog.csdn.net/qq_22206395/article/details/108240252)

根据tokenizer生成的token_type_ids（也就是segment_ids）生成attention mask矩阵。

先使用**torch.cumsum**函数按*列*进行相加，然后与前一位比大小，就会生成阶梯形的矩阵。

```
>>> segment_ids
torch.tensor([[0,0,0,1,1]])
>>> idxs 
tensor([[0, 0, 0, 1, 2]])
>>> idxs[:, None, :] 
tensor([[[0, 0, 0, 1, 2]]])
>>> idxs[:, :, None]  
tensor([[[0],
         [0],
         [0],
         [1],
         [2]]])
>>> mask  
tensor([[[ True,  True,  True, False, False],
         [ True,  True,  True, False, False],
         [ True,  True,  True, False, False],
         [ True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True]]])
```

虽然[huggingface](https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/bert#transformers.BertForMaskedLM)文档里写attention_mask写shape为二维，但其实是可以输入三维的矩阵的。

> attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional)

### 数据处理

将爬取的小说句子组合并一一对应，并在每句句子结尾加上自己设置的[EOS]符号，然后使用tokenizer进行编码，add_special_tokens设置为False。

**特殊token**可在**tokenizer.json**里设置。

### apex加速

为了帮助提高Pytorch的训练效率，英伟达提供了混合精度训练工具Apex。号称能够在不降低性能的情况下，将模型训练的速度提升2-4倍，训练显存消耗减少为之前的一半。

*参考：*
- [一文详解Apex的安装和使用教程（一款基于 PyTorch 的混合精度训练加速神器）](https://blog.csdn.net/mrjkzhangma/article/details/100704397)
- [nvidia训练深度学习模型利器apex使用解读](https://blog.csdn.net/benben044/article/details/125649981?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-125649981-blog-109579122.235%5Ev38%5Epc_relevant_default_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-125649981-blog-109579122.235%5Ev38%5Epc_relevant_default_base)

## 生成

一个字一个字地生成，直到生成[EOS]符号或超过一定数量。

## 部署

使用flask框架进行部署。
